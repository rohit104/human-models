\section{Experimental results}
\label{sec:experiments}

We conducted our experiments using Amazon Mechanical
Turk~\footnote{http://www.mturk.com.}, which allows workers (our pool
of prospective subjects) to perform small jobs for a fee through a Web
interface.  No specialized training or knowledge is typically expected
of the workers.  Amazon Mechanical Turk has been successfully used in
the past to develop gold-standard data for natural language
processing~\cite{snow-08} and to label images~\cite{imagenet-cvpr09}.

we prepare two randomly-chosen, 100-document subsets of English
Wikipedia~\footnote{http://en.wikipedia.org}.  For convenience, we
denote these two sets of documents as \emph{set1} and \emph{set2}.
For each document, we keep only the first 150 words for our
experiments.  Because of the encyclopedic nature of the corpus, the
first 150 words typically provides a broad overview of the themes in
the article.  We also removed from the corpus stop words and words
which occur infrequently\footnote{Infrequently occurring words were
  identified as those appearing fewer than eight times on a larger
  collection of 7726 articles.}, leading to a lexicon of 8263 words.
After this pruning \emph{set1} contained 11614 words and \emph{set2}
contained 11318 words.

Workers were asked to perform twenty of the taggings described in
\mysec{sec:tasks} for each task; workers were paid \$0.25 for each
such task.  The number of latent topics, $K$, is a free parameter.
Here we explore two values of this parameter, $K=10$ and $K=15$,
leading to a total of four experiments --- two for each set of
documents and two for each value of $K$.

\subsection{Tagging behavior}
\label{sec:evalmetrics}

There are several metrics commonly used to evaluate topic models in
the literature~\cite{wallach-09}.  Many of these metrics are
\emph{predictive} metrics; that is, they capture the model's ability
to predict a \emph{test set} of unseen documents after having learned
its parameters from a \emph{training set}.  In this work, we set aside
20\% of the documents in each corpus as a test set and train on the
remaining 80\% of documents.  We then compute predictive rank and
predictive log likelihood.

To ensure consistency of evaluation across different models, we follow
Teh et al.'s~\cite{TehKurWel2008} approximation of the predictive
likelihood $p(\textbf{w}_d|D_{\textrm{train}})$ using $p({\bm
  w}_d|D_{\textrm{train}}) \approx p({\bm w}_d|\hat{\theta}_d)$, where
$\hat{\theta}_d$ is a point estimate of the posterior topic
proportions for document $d$. For pLSI $\hat{\theta}_d$ is the MAP
estimate; for LDA and CTM $\hat{\theta}_d$ is the mean of the
variational posterior.
With this information, we can ask what words the model believes will
be in the document and compare it with the document's actual
composition.  Given document $\bm w_d$, we first estimate
$\hat{\theta}_d$ and then for every word in the vocabulary, we compute
$p(w|\hat{\theta}_d) = \sum_z p(w|z)p(z|\hat{\theta}_d)$.  Then we
compute the average rank for the terms that actually appeared in document
$\bm w_d$ (we follow the convention that lower rank is better).

The average word likelihood and average rank across all documents in
our test set are shown in Table~\ref{tab:word-prediction}.  These
results are consistent with the values reported in the
literature~\cite{blei-03,blei-06}; in most cases CTM performs best,
followed by LDA.

\begin{table*}
  \caption{The five words with the highest probability mass in each topic inferred by humans using the task described in \mysec{sec:tasks}.  Each subtable shows the results for a particular experimental setup.   Each row is a topic; the most probable words are ordered from left to right.}
\label{tab:topic-samples}
\centering
\footnotesize
\hspace*{-.4in}
\subfigure[\emph{set1}, $K = 10$]{
\begin{tabular}{lllll}
  railway & lighthouse & rail & huddersfield & station \\ 
  school & college & education & history & conference \\ 
  catholic & church & film & music & actor \\ 
  runners & team & championships & match & racing \\ 
  engine & company & power & dwight & engines \\ 
  university & london & british & college & county \\ 
  food & novel & book & series & superman \\ 
  november & february & april & august & december \\ 
  paint & photographs & american & austin & black \\ 
   war & history & army & american & battle \\ 
\end{tabular}
}%
\subfigure[\emph{set2}, $K = 10$]{
  \begin{tabular}{|lllll}
    president & emperor & politician & election & government \\ 
    american & players & swedish & team & zealand \\ 
    war & world & navy & road & torpedo \\ 
    system & pop & microsoft & music & singer \\ 
    september & 2007 & october & december & 1999 \\ 
    television & dog & name & george & film \\ 
    people & malay & town & tribes & cliff \\ 
    diet & chest & enzyme & hair & therapy \\ 
    british & city & london & english & county \\ 
    school & university & college & church & center \\ 
  \end{tabular}
}
\hspace*{-.5in}
\subfigure[\emph{set1}, $K = 15$]{
\begin{tabular}{lllll}
australia & knee & british & israel & set \\ 
  catholic & roman & island & village & columbia \\ 
  john & devon & michael & austin & charles \\ 
  school & university & class & community & district \\ 
  november & february & 2007 & 2009 & 2005 \\ 
  lighthouse & period & architects & construction & design \\ 
  railway & rail & huddersfield & ownership & services \\ 
  cyprus & archdiocese & diocese & king & miss \\ 
  carson & gordon & hugo & ward & whitney \\ 
  significant & application & campaign & comic & considered \\ 
  born & london & american & england & black \\ 
  war & defense & history & military & artillery \\ 
  actor & film & actress & band & designer \\ 
  york & michigan & florida & north & photographs \\ 
  church & catholic & county & 2001 & agricultural \\ 
\end{tabular}
}%
\subfigure[\emph{set2}, $K = 15$]{
\begin{tabular}{|lllll}
  music & pop & records & singer & artist \\ 
  film & paintings & movie & painting & art \\ 
  school & university & english & students & british \\ 
  drama & headquarters & chess & poet & stories \\ 
  family & church & sea & christmas & emperor \\ 
  dog & broadcast & television & bbc & breed \\ 
  champagne & regular & character & characteristic & common \\ 
  election & government & parliament & minister & politician \\ 
  enzyme & diet & protein & hair & oxygen \\ 
  war & navy & weapons & aircraft & military \\ 
  september & october & december & 2008 & 1967 \\ 
  district & town & marin & america & american \\ 
  car & power & system & device & devices \\ 
  hockey & players & football & therapy & champions \\ 
  california & zealand & georgia & india & kolkata \\ 
\end{tabular}
}
\vspace{0.2in}
\end{table*}

\begin{figure*}
\centering
\subfigure[$K = 10$]{\includegraphics[width = .47\linewidth]{figures/entropy_K10}}%
\subfigure[$K = 15$]{\includegraphics[width = .47\linewidth]{figures/entropy_K15}}

\caption{A comparison of the entropy of distributions drawn from a
  Dirichlet versus the entropy of the topic proportions inferred by
  workers.  Each column of the boxplot shows the distribution of
  entropies for 100 draws from a Dirichlet distribution with parameter
  $\alpha$.  The two rightmost columns show the distribution of the
  entropy of the topic proportions inferred by workers on \emph{set1}
  and \emph{set2}.  The $\alpha$ workers typically falls between 0.2
  and 0.5.}

\label{fig:entropy}
\end{figure*}

\subsection{Comparison with LDA}

The tasks described in \mysec{sec:tasks} were offered on Amazon
Mechanical Turk (http://www.mturk.com), 
For both the word intrusion and topic intrusion tasks, we presented
each worker with jobs containing ten of the tasks described in
\mysec{sec:tasks}.

As described in \mysec{sec:wordintrusion}, the word intrusion task
measures how well the inferred topics match human concepts (using
\emph{model precision}, i.e., how well the intruders detected by the
subjects correspond to those injected into ones found by the topic model).  

Let $\omega^{m}_{k}$ be the index of the intruding word among the words generated
from the $k^{th}$ topic inferred by model $m$.  Further let $i^m_{k, s}$ be
the intruder selected by subject $s$ on the set of words generated from the
$k$th topic inferred by model $m$ and let $S$ denote the number of subjects.  We
define model precision by the fraction of subjects agreeing with the model,
\begin{equation}
  \mathrm{MP}^m_k = \textstyle \sum_{s} \mathds{1}(i^m_{k,s} = \omega^{m}_{k}) / S.
  \label{eq:mp}
\end{equation}


\myfig{fig:precision} shows boxplots of the precision for the three
models on the two corpora.  In most cases LDA performs best. Although
CTM gives better predictive results on held-out likelihood, it does
not perform as well on human evaluations. This may be because CTM
finds correlations between topics and correlations within topics are
confounding factors; the intruder for one topic might be selected from
another highly correlated topic.  The performance of pLSI degrades
with larger numbers of topics, suggesting that
overfitting~\cite{blei-03} might affect interpretability as well as
predictive power.

\myfig{fig:topic_precision} (left) shows examples of topics with high
and low model precisions from the NY Times data fit with LDA using 50
topics. In the example with high precision, the topic words all
coherently express a painting theme.  For the low precision example,
 ``taxis'' did not fit in with the other
political words in the topic, as $87.5\%$ of subjects chose ``taxis''
as the intruder.


The relationship between model precision, $\mathrm{MP}^m_k$, and the
model's estimate of the likelihood of the intruding word in
\myfig{fig:prec_vs_lhood} (top row) is surprising.  The highest
probability did not have the best interpretability; in fact, the trend
was the opposite.  This suggests that as topics become more
fine-grained in models with larger number of topics, they are less
useful for humans.  The downward sloping trend lines in
\myfig{fig:prec_vs_lhood} implying that the models are often trading
improved likelihood for lower interpretability.


The model precision showed a negative correlation (Spearman's $\rho =
-0.235$ averaged across all models, corpora, and topics) with the
number of senses in WordNet of the words displayed to the
subjects~\cite{Miller90} and a slight positive correlation ($\rho =
0.109$) with the average pairwise Jiang-Conrath similarity of
words\footnote{Words without entries in WordNet were ignored; polysemy
  was handled by taking the maximum over all senses of words.  To
  handle words in the same synset (e.g. ``fight'' and ``battle''), the
  similarity function was capped at 10.0.}~\cite{jiang-97}.  

