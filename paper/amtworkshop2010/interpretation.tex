\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2010}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath,amsthm, amssymb}
\usepackage{multirow}
\usepackage{microtype}
\usepackage{subfigure}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{bm}
\usepackage{multirow}
\usepackage{dsfont}

%%%% EVIL %%%%%%
\usepackage[compact,small]{titlesec}
\usepackage[small]{caption}
\usepackage{mdwlist}

\renewcommand{\topfraction}{0.95}
\renewcommand{\textfraction}{0.05}
\renewcommand{\floatpagefraction}{0.95}

\setlength{\abovecaptionskip}{0pt}

%\setlength{\belowcaptionskip}{0pt}
\setlength{\textfloatsep}{8pt}
%\setlength{\floatsep}{0pt}
%%%% EVIL %%%%%%

\newcommand{\myfig}[1]{Figure~\ref{#1}}
\newcommand{\myeq}[1]{Equation~\ref{#1}}
\newcommand{\mytab}[1]{Table~\ref{#1}}
\newcommand{\wordset}[1]{\texttt{\{#1\}}}
\newcommand{\word}[1]{\texttt{#1}}
\newcommand{\mysec}[1]{Section~\ref{#1}}

%%% Temporary
%\usepackage{fancyhdr}
%\chead{{\bf DRAFT COPY: DO NOT CITE OR DISTRIBUTE}}
%\lhead{}
%\rhead{}
%\pagestyle{fancy}

\title{Not-So-Latent Dirichlet Allocation: \\ 
       Collapsed Gibbs Sampling Using Human Judgments}

\author{}

\begin{document}

%% \makeanontitle
\maketitle
\vspace{-.1in}
\begin{abstract}%
  Probabilistic topic models are a popular tool for the unsupervised
  analysis of text, providing both a predictive model of future text
  and a latent topic representation of the corpus.  Recent studies
  have found that while there are suggestive connections between topic
  models and the way humans interpret data, these two often disagree.
  In this paper, we explore this disagreement from the perspective of
  the learning process rather than the output.  We present a novel
  task, \emph{tag-and-cluster}, which asks subjects to simultaneously
  annotate documents and cluster those annotations.  We use these
  annotations as a novel approach for constructing a topic model,
  grounded in human interpretations of documents.  We demonstrate that
  these topic models have features which distinguish them from
  traditional topic models.
\end{abstract}

\input{intro}
\input{related}
\input{tasks}
\input{experiments}
\section{Discussion}
  We presented the first method for constructing topic models using
  human judgments.  Our approach relies on a novel task,
  \emph{tag-and-cluster}, which asks users to simultanesouly annotate
  a document with one of its words and to cluster those annotations.
  We demonstrate using experiments on Amazon Mechanical Turk that our
  method constructs topic models quickly and robustly.  We also show
  that while our topic models bear many similarities to traditionally
  constructed topic models, our human-learned topic models have unique
  features such as fixed sparsity and a tendency to construct topics
  around concepts which models such as LDA typically fail to find.

  We also underscore that the collapsed Gibbs sampling framework is
  expressive enough to use as the basis for human-guided topic model
  inference.  This may motivate, as future work, the construction of
  different modeling assumptions which lead to sampling equations
  which more closely match the empirically observed sampling performed
  by humans.  In effect, our method constructs a series of samples
  from the posterior, a gold standard which future topic models can
  aim to emulate.
\bibliographystyle{naaclhlt2010}

\small

\bibliography{journal-abbrv,nlp}

\end{document}
