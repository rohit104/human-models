\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2010}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath,amsthm, amssymb}
\usepackage{multirow}
\usepackage{microtype}
\usepackage{subfigure}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{bm}
\usepackage{multirow}
\usepackage{dsfont}

%%%% EVIL %%%%%%
\usepackage[compact,small]{titlesec}
\usepackage[small]{caption}
\usepackage{mdwlist}

\renewcommand{\topfraction}{0.99}
\renewcommand{\textfraction}{0.01}
\renewcommand{\floatpagefraction}{0.99}

\setlength{\abovecaptionskip}{0pt}

%\setlength{\belowcaptionskip}{0pt}
\setlength{\textfloatsep}{8pt}
%\setlength{\floatsep}{0pt}
%%%% EVIL %%%%%%

\newcommand{\myfig}[1]{Figure~\ref{#1}}
\newcommand{\myeq}[1]{Equation~\ref{#1}}
\newcommand{\mytab}[1]{Table~\ref{#1}}
\newcommand{\wordset}[1]{\texttt{\{#1\}}}
\newcommand{\word}[1]{\texttt{\small #1}}
\newcommand{\mysec}[1]{Section~\ref{#1}}

%%% Temporary
%\usepackage{fancyhdr}
%\chead{{\bf DRAFT COPY: DO NOT CITE OR DISTRIBUTE}}
%\lhead{}
%\rhead{}
%\pagestyle{fancy}

\title{\vspace{-0.4in}Not-So-Latent Dirichlet Allocation: \\ 
       Collapsed Gibbs Sampling Using Human Judgments}

\author{Jonathan Chang \\ 
        Facebook \\ 
        1601 S. California Ave. \\ 
        Palo Alto, CA 94304 \\ 
        \texttt{jonchang@facebook.com}}

\begin{document}
%% \makeanontitle
\maketitle
\vspace{-.1in}
\begin{abstract}%
  Probabilistic topic models are a popular tool for the unsupervised
  analysis of text, providing both a predictive model of future text
  and a latent topic representation of the corpus.  Recent studies
  have found that while there are suggestive connections between topic
  models and the way humans interpret data, these two often disagree.
  In this paper, we explore this disagreement from the perspective of
  the learning process rather than the output.  We present a novel
  task, \emph{tag-and-cluster}, which asks subjects to simultaneously
  annotate documents and cluster those annotations.  We use these
  annotations as a novel approach for constructing a topic model,
  grounded in human interpretations of documents.  We demonstrate that
  these topic models have features which distinguish them from
  traditional topic models.
\end{abstract}

\input{intro}
\input{related}
\input{tasks}
\input{experiments}
\section{Discussion}
We presented a new method for constructing topic models using human
judgments.  Our approach relies on a novel task,
\emph{tag-and-cluster}, which asks users to simultaneously annotate a
document with one of its words and to cluster those annotations.  We
demonstrate using experiments on Amazon Mechanical Turk that our
method constructs topic models quickly and robustly.  We also show
that while our topic models bear many similarities to traditionally
constructed topic models, our human-learned topic models have unique
features such as fixed sparsity and a tendency for topics to be constructed
around concepts which models such as LDA typically fail to find.

  We also underscore that the collapsed Gibbs sampling framework is
  expressive enough to use as the basis for human-guided topic model
  inference.  This may motivate, as future work, the construction of
  different modeling assumptions which lead to sampling equations
  which more closely match the empirically observed sampling performed
  by humans.  In effect, our method constructs a series of samples
  from the posterior, a gold standard which future topic models can
  aim to emulate.
\section*{Acknowledgments}
The author would like to thank Eytan Bakshy and Professor Jordan
Boyd-Graber-Ying for their helpful comments and discussions.

\bibliographystyle{naaclhlt2010}
{\footnotesize
\bibliography{journal-abbrv,nlp}
}
\end{document}

% Figure 4:
% Also, you should label your axes.

